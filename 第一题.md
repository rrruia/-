- #### **relu激活函数与sigmoid和tanh有哪些优势**

  1. sigmoid和tanh激活函数在输入值（或者参数更新后某个神经元的输入值）绝对值较大时导数接近0，导致梯度下降难以更新参数（deta w也接近0），也就是梯度消失；而relu函数在正区间上导数恒为1，避免了这一点

  2. sigmoid和tanh激活函数涉及到指数运算，算力资源消耗多，而relu函数仅涉及一次的计算，算力消耗少，计算速度快

  3. 形状上非常简单，但是组合后又可以拟合任意复杂的函数

     

- #### **解释梯度消失和梯度爆炸，并给出你觉的有效的解决方法** 

  **梯度消失**：梯度下降过程中参数反向传播的导数值接近0，导致参数难以更新。常见于激活函数导数值总是小于1，根据链式法则，在多次反向传播过程中导致导数传递到第一层神经元时接近0，使第一层参数更新幅度过小，如sigmoid函数（导数恒小于0.25），和tanh函数。

  **梯度爆炸**：和梯度消失相反，梯度下降过程中参数的导数值过大，导致参数变化过大，产生震荡，难以收敛。常见于激活函数导数值大于1并且神经网络过深，或者初始参数过大。总体而言都是导数通过链式法则层层放大的结果。

  **解决方法**：

  1. 防止神经网络层数过深

  2. 对于梯度消失，改用更合适的激活函数（如从sigmoid改成relu）

  3. 对数据集进行归一化，让数据分布在合适的区间

  4. 对参数进行L1 L2正则化,调整缩小w的值，间接控制梯度规模

  5. 权重初始化（Xavier初始化：sigmoid，tanh；He初始化：RELU）

  6. 对于梯度爆炸，进行梯度裁剪，在梯度超过阈值的时候强行设置为阈值

     

- #### **讨论你对正则化和规范化的认识** 

  

  **正则化**广义上指可以防止模型过拟合的方法

  **L1或L2正则化**方法是在损失函数上加了一个正则化式（L1或L2），这个式子可以体现w的个数和大小，从而在反向传播更新参数的过程中防止w因为个别数据过大，而产生过拟合。这样拟合后可以表示函数整体规律，而不是记住噪声。

  其中，L1=|w1|+|w2|+...+|wn|,L2=根号下（w1** 2+w2** 2+...+wn** 2）

  **dropout正则化**是在每次训练的前向传播中，随机让一部分神经元失活，防止训练过度依赖局部特征，从而表现函数整体规律，减少极端值对拟合的影响。

  **Early Stop早停** 让训练在验证集连续几轮训练loss未得到优化时停止，并取loss最小值对应参数，从而防止模型训练过度而过拟合；同时这个优化程度也就是损失降低值需要大于一个值，否则不算优化了

  **数据增强** 对数据进行同语义的变形，产生更多数据，提高模型泛化能力和鲁棒性，也可以减少过拟合

  **权重约束** 与L1 L2正则化相似而不同，后者是对参数过大过多进行惩罚，也就是软约束；而前者是直接把参数限定在某个范围内，相当于硬约束

  

  **规范化**是将数据按一定规律缩放到一个固定的范围，这种规律遵从正态分布（批归一化是标准正态分布）或者均匀分布

  **作用**：使不同特征的数值范围和分布更加一致，从而让梯度收敛更稳定更快；也可以使数据不落入激活函数的饱和区间

  可以作用在**数据预处理**（Z-score标准化、min-max归一化）时，也可以作用在模型**训练过程中**（BatchNorm,LayerNorm,GroupNorm）

  其中，Z-score标准化是让数据呈现（0,1）的高斯分布，min-max归一化是把数据区间缩放到[0,1]或[-1,1]

  

  同时，归一化也能起到正则化的效果

  

- #### **解释批量梯度下降和随机梯度下降的作用和关系**  

  **批量梯度下降**：一次性把所有样本输入网络进行训练

  **随机梯度下降**：每次只把一个样本输入网络并训练，每计算一个样本就更新参数

  **关系**：都是梯度下降中传递参数的方法，区别在于单次训练样本数量选择。批量梯度下降计算开销大且慢，随机梯度下降收敛效果差，因此常中和两者选择小批量梯度下降。

  

- #### **深度学习中Epoch、Batch和Iteration都是什么** 

  epoch：将样本集中每个样本（全部样本）进行一次正向传播和反向传播的次数

  batch：划分的每次训练（一次正向传播和反向传播）的样本子集

  interation：一次参数更新，即处理一个batch并更新参数的动作

  

- #### **谈谈你对卷积神经网络的理解，并讨论对于cnn具体参数都是什么和如何选**

  **卷积神经网络**常用来处理图像识别问题，具有卷积核kernal，填充padding，池化pooling，步幅stride，通道数channels等参数；因为卷积核是二维数组，所以可以比较抽象地提取图像的多种特征，最终将抽象出来的多种特征扁平化处理（flatten），与全连接层相接，从而识别图像

  其**结构**有，输入，（卷积层，激活函数，池化层）*n，卷积层，激活函数，扁平化，全连接层，输出

  卷积核kernal：相当于DNN中的权重，用来提取图像多角度的特征

  填充padding：在输入数据周围填充0，防止图像边缘特征被忽略

  池化pooling：压缩图片数据，仅反应图像最突出的特点；分为最大池化和平均池化

  步幅stride：卷积核每次计算移动步幅

  通道数channels：每层学习的特征数量，由上一层卷积核个数决定

  **如何选择**：对于不同项目目标选择不同的参数搭配

  默认情况下，**卷积层**选择卷积核3×3，stride=1，padding使保持原有输入尺寸，通道数增加一倍或更多；

  **池化层**选择卷积核2×2，stride=2，无填充；

  **隐藏层激活函数**的选择上，通常使用Relu激活函数或者对其优化的leaky relu、gelu激活函数，对隐藏层较少的神经网络也可使用sigmoid和tanh激活函数；

  **输出层激活函数**的选择上，应以目的为导向，二分类问题使用sigmoid，多分类问题使用softmax，多标签问题用sigmoid对每个类别概率进行计算，线性回归问题使用线性函数；

  选用**小批量梯度下降**，batchsize通常在64，128，256

  **学习率**应随batchsize增大而增大，通常在0.001～0.01之间；

  **momentum**默认为0.9；

  

- #### **1*1的卷积核有什么作用** 

  1. 调整通道数，进行特征图的降维（降低模型复杂度和计算量）或升维
  2. 如果后接激活函数，可以增加神经网络的非线性表达能力
  3. 降维时可以将神经网络进行线性组合，融合通道特征
  4. 减少计算量和参数量

- #### **解释感受野这一概念**

  是指网络中某一层神经元所看到的输入图片的像素范围，由一层层神经元逐步累加

  分为理论感受野和有效感受野，理论感受野是计算出来的计算机最大考察像素范围和视域，有效感受野是实际上计算机考虑的像素范围，分布在理论感受野中间区域，可以由反向传播得到

  可以通过池化层快速增加感受野来处理高像素图片

  计算公式是 上一层的感受野＋（卷积核大小-1）×之前每一层步幅的累乘，而每层池化层会把感受野翻倍

- #### **卷积神经网络中池化的作用** 

  1. 指数级增加感受野，便于处理高像素图片
  2. 保留、汇聚显著特征，让模型更加关注特征是否存在而不是具体位置，让特征的学习更有普适性
  3. 减小了下一层的输入大小，从而减小计算量和参数数
  4. 减少噪声对模型训练的干扰，关注主要特征，从而一定程度防止过拟合

- #### **讨论学习率衰减策略的方法以及解释原因** 

  学习率衰减策略是让学习率在训练初期较大，训练后期较小的策略

  方法：

  ##### 固定学习率衰退

  1. 指数衰减，参考衰减公式ηt=η0⋅e−kt

  2. 固定步长衰减，每隔一定step降低一次学习率，降低幅度与原有学习率正相关

  3. 多步长衰减，与固定步长相似，但是对于不同区间采用不同更新频率，有的区间甚至不进行更新

  4. 余弦退火，有多个学习率下降周期（跳出局部最优，防止提前的‘假收敛’），一个周期内学习率先缓慢下降，再快速下降，最后缓慢下降，与余弦图像相似，而且每个周期的长度会越来越长（前面的周期收敛更快，后面的周期收敛更稳定）

     另外，可以使用warmup让初始学习过程稳定，避免剧烈震荡

  ##### 自适应学习率衰减

  1. Adagrad，学习率的分子部分为常数，分母部分与  参数的历史梯度平方的累计和 正相关，因为累计和单调增加，所以学习率单调减少。学习率衰退速度与每一步参数梯度相关。
  2. RMSProp，与adagrad相似，但是分母项不是对历史梯度的简单累加，而是这一步梯度平方与上一步分母项的加权和（加权系数和为1），更关心梯度更新中最近的行为
  3. Adam，与RMSProp相似

  原因：

  训练初期学习率较大可以加速收敛，后期学习率较小可以更靠近最低点，收敛效果更好更稳定

  学习率衰减还可以帮助减缓过拟合，前期较大学习率不会拟合噪声，后期较小学习率防止模型记住细节噪声，让模型更泛化

- #### **解释动量的含义和作用** 

  动量是一种梯度下降过程中对梯度项的优化，不仅考虑了当前位置的梯度，还一定程度参考了过去运动的趋势（过去累计的梯度）

  作用：

  1. 可以通过鞍点，因为有过去累计的梯度
  2. 可以较快通过缓坡阶段
  3. 可以逃离局部最小值（如果过去积累的梯度未消耗完）
  4. 收敛效果更好，过去梯度与当前梯度的相互抵消可以避免目标点左右震荡
  5. 总体上减少学习过程中的震荡

- #### **残差连接有什么意义和作用** 

  1. 缓解梯度消失问题，每一层梯度比没有残差连接的梯度多1
  2. 利于网络优化，让loss曲面更光滑，收敛更稳定
  3. 由于缓解梯度消失和收敛稳定性，让深层神经网络变成可能，神经网络参数量增大，拟合效果更好，准确率更高
  4. 不丢失原有输入数据，防止loss不减反增
  5. 学习过程不是从0到1的生成，而是从1到2的修改，更简单，效率和准确率也更高

- #### **BatchNorm和LayerNorm的区别及各自优势**

**batchnorm**是在一批样本内，对同一特征维度的所有样本进行归一化（使用当前batch的均值和方差做 Z-score 标准化）

<img src="https://picx.zhimg.com/80/v2-fff469028b750a31598e272fd99968c3_1440w.webp" alt="img" style="zoom: 33%;" />

**layernorm**是在每个样本内部，对这一个样本的所有特征进行归一化（使用当前样本所有特征的均值和方差做 Z-score 标准化）

<img src="https://picx.zhimg.com/80/v2-2fde3f1da918cf79fabb2df348ac8f39_1440w.webp" alt="img" style="zoom:33%;" />

**对于batchnorm**，因为每批样本各不相同，均值与标准差也各不相同，如果batchsize较小，每批样本归一化的结果相差较大，会造成训练效果不稳定；一定程度上增加鲁棒性，减少过拟合；

可以将尺度不同的特征压缩到相似的区间上，消除不同特征之间的大小关系，但是保留了不同样本间的大小关系，让训练不受特征间数值大小的影响；

适用于batchsize较大，特征依赖不同样本间的统计参数时，如CNN中

**而对于layernorm**，每个样本的归一化相互独立，所以不受batchsize影响；

消除了不同样本间的大小关系，但是保留了一个样本内不同特征之间的大小关系；在如NLP语义处理上，因为词序对语义理解影响不大，更在意句子中每个词的分布，所以更适用layernorm；相似场景也更适用layernorm；

不在意batch大小，适用于依赖同一样本内不同参数的关系时，如NLP，RNN中